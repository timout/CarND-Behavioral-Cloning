{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import io, os\n",
    "import functools\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda, SpatialDropout2D, ELU\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Cropping2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "try_prefix = 'try'\n",
    "log_file = 'driving_log.csv'\n",
    "img_dir = 'IMG'\n",
    "train_file = os.path.join(data_dir, 'train.csv')\n",
    "validation_file = os.path.join(data_dir, 'validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tryouts(data_dir, prefix):\n",
    "    filenames = os.listdir(data_dir)\n",
    "    return sorted(list(map(lambda d: os.path.join(data_dir, d), filter(lambda s: s.startswith(prefix), filenames))))\n",
    "\n",
    "def fix_logs_paths(img_dir, data_frame):\n",
    "    pathfn = lambda p: os.path.join(img_dir, p.split('/')[-1])\n",
    "    data_frame.center = data_frame.center.apply(pathfn)\n",
    "    data_frame.left = data_frame.left.apply(pathfn)\n",
    "    data_frame.right = data_frame.right.apply(pathfn)\n",
    "    \n",
    "def load_log(log_dir, log_file, img_dir):\n",
    "    f = os.path.join(log_dir, log_file)\n",
    "    df = pd.read_csv(f, header=None, names=['center','left','right', 'angle', 'throttle', 'break', 'speed'])\n",
    "    i = os.path.join(log_dir, img_dir)\n",
    "    fix_logs_paths(i, df)\n",
    "    return df\n",
    "        \n",
    "def merge_logs(dfs):\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def prepare_log():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    tryouts = find_tryouts(data_dir, try_prefix)\n",
    "    log_frames = list(map(lambda t: load_log(t, log_file, img_dir), tryouts))\n",
    "    log = merge_logs(log_frames)\n",
    "    rev_log = log.copy()\n",
    "    log['r'] = True\n",
    "    rev_log['r'] = False\n",
    "    merged_log = merge_logs(log, rev_log)\n",
    "    merged_log = merged_log.reindex(np.random.permutation(merged_log.index))\n",
    "    train_samples, validation_samples = train_test_split(merged_log, test_size=0.2)\n",
    "    train_samples = train_samples.reindex(np.random.permutation(train_samples.index))\n",
    "    validation_samples = validation_samples.reindex(np.random.permutation(validation_samples.index))\n",
    "    train_samples.to_csv(train_file)\n",
    "    validation_samples.to_csv(validation_file)\n",
    "    return len(train_samples), len(validation_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(file_name, batch_size):\n",
    "    from sklearn.utils import shuffle\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        chunk_iter = pd.read_csv(file_name, chunksize=batch_size)\n",
    "        for chunk in chunk_iter:\n",
    "            images = []\n",
    "            angles = []\n",
    "            for row in chunk.itertuples():\n",
    "                img = cv2.imread(row.center)\n",
    "                ang = float(row.angle)\n",
    "                images.append(img)\n",
    "                angles.append(ang)\n",
    "            \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_cropped(img):\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(img, (40, 160))\n",
    "\n",
    "def resize_img(img):\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(img, (80, 160))\n",
    "\n",
    "def create_model1():\n",
    "    model = Sequential([\n",
    "        Lambda(resize_img, input_shape=(160, 320, 3)),\n",
    "        Lambda(lambda x: x/127.5 - 1.),\n",
    "        Convolution2D(16, (5, 5), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Convolution2D(32, (5, 5), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])   \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def create_model2():\n",
    "    model = Sequential([\n",
    "        Lambda(resize_img, input_shape=(160, 320, 3)),\n",
    "        Lambda(lambda x: x/127.5 - 1.),\n",
    "        Convolution2D(16, (8, 8), strides=(4, 4), padding=\"same\"),\n",
    "        ELU(),\n",
    "        Convolution2D(32, (5, 5), strides=(2, 2), padding=\"same\"),\n",
    "        ELU(),\n",
    "        Convolution2D(64, (5, 5), strides=(2, 2), padding=\"same\"),\n",
    "        Flatten(),\n",
    "        Dropout(.5),\n",
    "        ELU(),\n",
    "        Dense(512),\n",
    "        Dropout(.7),\n",
    "        ELU(),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def create_model3():\n",
    "    model = Sequential([\n",
    "        Lambda(lambda x: x/127.5 - 1., input_shape=(160, 320, 3)),\n",
    "        Cropping2D(cropping=((70, 25), (0, 0))),\n",
    "        Convolution2D(24, (5, 5), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Convolution2D(36, (5, 5), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Convolution2D(48, (5, 5), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Convolution2D(64, (3, 3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(100),\n",
    "        Dense(50),\n",
    "        Dense(10),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def create_model4():\n",
    "    model = Sequential([\n",
    "        Cropping2D(cropping=((60, 20), (0, 0)), input_shape=(160, 320, 3)),\n",
    "        Lambda(resize_cropped),\n",
    "        Lambda(lambda x: x/127.5 - 1.),\n",
    "        Convolution2D(16, (5, 5), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Convolution2D(32, (5, 5), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Convolution2D(64, (5, 5), activation='relu', padding=\"same\"),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])   \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, filename):\n",
    "    model.save(filename)\n",
    "    print(\"model saved, filename: {}.\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mt = create_model1()\n",
    "ldm = load_model('model2.h5.copy')\n",
    "for layer in ldm.layers:\n",
    "    layer.trainable = False\n",
    "#ldm.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_model(model, filename):\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCH = 32\n",
    "    train_size, validation_size = prepare_log()\n",
    "    train_generator = generator(train_file, BATCH_SIZE)\n",
    "    validation_generator = generator(validation_file, BATCH_SIZE)\n",
    "    model.fit_generator(train_generator, \n",
    "                        steps_per_epoch=train_size/BATCH_SIZE, \n",
    "                        validation_data=validation_generator, \n",
    "                        validation_steps=validation_size/BATCH_SIZE, \n",
    "                        epochs=EPOCH)\n",
    "    save_model(model, filename)  \n",
    "   \n",
    "\n",
    "#models = [create_model1() ,create_model2(), create_model3(), create_model4()] \n",
    "#for i, m in enumerate(models):\n",
    "#    m_name = \"model{}.h5\".format(i)\n",
    "#    run_model(m, m_name) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
