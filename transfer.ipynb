{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tabakumov/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import io, os\n",
    "import functools\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda, SpatialDropout2D, ELU\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Cropping2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tryouts(data_dir, prefix):\n",
    "    \"\"\" \n",
    "    Find all tryouts dirs under data_dir, all tryout dirs must start with prefix \n",
    "    and returns list of dirs with training data\n",
    "    \"\"\"\n",
    "    filenames = os.listdir(data_dir)\n",
    "    return sorted(list(map(lambda d: os.path.join(data_dir, d), filter(lambda s: s.startswith(prefix), filenames))))\n",
    "\n",
    "def fix_logs_paths(img_dir, data_frame):\n",
    "    \"\"\" Fix paths to images in a log dataframe \"\"\"\n",
    "    pathfn = lambda p: os.path.join(img_dir, p.split('/')[-1])\n",
    "    data_frame.center = data_frame.center.apply(pathfn)\n",
    "    data_frame.left = data_frame.left.apply(pathfn)\n",
    "    data_frame.right = data_frame.right.apply(pathfn)\n",
    "    \n",
    "def load_log(log_dir, log_file, img_dir):\n",
    "    \"\"\" Load tryout csv file (driving_log.csv) \"\"\"\n",
    "    f = os.path.join(log_dir, log_file)\n",
    "    df = pd.read_csv(f, header=None, names=['center','left','right', 'angle', 'throttle', 'break', 'speed'])\n",
    "    i = os.path.join(log_dir, img_dir)\n",
    "    fix_logs_paths(i, df)\n",
    "    return df\n",
    "        \n",
    "def merge_logs(dfs):\n",
    "    \"\"\" Merge loaded dataframes for evere 'driving_log.csv' \"\"\"\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def prepare_log(data_dir):\n",
    "    \"\"\" \n",
    "    Load 'driving_log.csv' for every 'tryNN' dir, merge, shuffle, split by 80/20 for training and validation.\n",
    "    Save resulted training and validation subsets to separate csv files - for generators\n",
    "    \"\"\" \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    try_prefix = 'try'           # Prefix for dirs with traning data\n",
    "    log_file = 'driving_log.csv' # CSV file with training data description, generated by simulator\n",
    "    img_dir = 'IMG'              # Directory with training images for every attempt, like ./data/try1/IMG\n",
    "    train_file = os.path.join(data_dir, 'train.csv') # Generated file with fixed training data split, needed for generator \n",
    "    validation_file = os.path.join(data_dir, 'validation.csv') # Generated file with fixed validation data split, needed for generator \n",
    "    \n",
    "    tryouts = find_tryouts(data_dir, try_prefix)\n",
    "    log_frames = list(map(lambda t: load_log(t, log_file, img_dir), tryouts))\n",
    "\n",
    "    #add records for flipped images, simple duplication, 'r'=True indicate fliiped image\n",
    "    log = merge_logs(log_frames)\n",
    "    rev_log = log.copy()\n",
    "    log['r'] = False\n",
    "    rev_log['r'] = True\n",
    "    merged_log = merge_logs([log, rev_log])\n",
    "\n",
    "    #shuffle\n",
    "    merged_log = merged_log.reindex(np.random.permutation(merged_log.index))\n",
    "    #split\n",
    "    train_samples, validation_samples = train_test_split(merged_log, test_size=0.2)\n",
    "    train_samples = train_samples.reindex(np.random.permutation(train_samples.index))\n",
    "    validation_samples = validation_samples.reindex(np.random.permutation(validation_samples.index))\n",
    "    #save to it training and validation sets\n",
    "    train_samples.to_csv(train_file)\n",
    "    validation_samples.to_csv(validation_file)\n",
    "    return len(train_samples), len(validation_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(file_name, batch_size):\n",
    "    \"\"\" \n",
    "    Generator to load train and validation data, Loads data in chunks (batch_size) \n",
    "    \"\"\"\n",
    "    from sklearn.utils import shuffle\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        chunk_iter = pd.read_csv(file_name, chunksize=batch_size)\n",
    "        for chunk in chunk_iter:\n",
    "            images = []\n",
    "            angles = []\n",
    "            for row in chunk.itertuples():\n",
    "                img = cv2.imread(row.center)\n",
    "                ang = float(row.angle)\n",
    "                # if 'r' flag is true flip the image\n",
    "                if row.r: \n",
    "                    img = np.fliplr(img)\n",
    "                    ang = -ang\n",
    "                images.append(img)\n",
    "                angles.append(ang)\n",
    "            \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    \"\"\" Create model \"\"\"\n",
    "    model = Sequential([\n",
    "        Lambda(lambda x: x/127.5 - 1., input_shape=(160, 320, 3)),\n",
    "        Cropping2D(cropping=((70, 25), (0, 0))),\n",
    "        Convolution2D(24, (5, 5), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Convolution2D(36, (5, 5), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Convolution2D(48, (5, 5), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Convolution2D(64, (3, 3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dropout(.2),\n",
    "        Dense(100),\n",
    "        Dropout(.2),\n",
    "        Dense(50),\n",
    "        Dropout(.2),\n",
    "        Dense(10),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])   \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def save_model(model, filename):\n",
    "    \"\"\" Save model \"\"\"\n",
    "    model.save(filename)\n",
    "    print(\"model saved, filename: {}.\".format(filename))\n",
    "\n",
    "def run_model(model, data_dir, batch_size=64, epochs=4):\n",
    "    \"\"\" Run model \"\"\"\n",
    "    train_size, validation_size = prepare_log(data_dir)\n",
    "    train_file = os.path.join(data_dir, 'train.csv') # Generated file with fixed training data split, needed for generator \n",
    "    validation_file = os.path.join(data_dir, 'validation.csv') # Generated file with fixed validation data split, needed for generator \n",
    "    train_generator = generator(train_file, batch_size)\n",
    "    validation_generator = generator(validation_file, batch_size)\n",
    "    return model.fit_generator(train_generator, \n",
    "                        steps_per_epoch=train_size/batch_size, \n",
    "                        validation_data=validation_generator, \n",
    "                        validation_steps=validation_size/batch_size, \n",
    "                        epochs=epochs,\n",
    "                        verbose=1)                   \n",
    "    \n",
    "def save_history(history, history_file):\n",
    "    \"\"\" Save model metrics \"\"\"\n",
    "    with open(history_file, 'wb') as f:\n",
    "        pickle.dump(history.history, f)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_fine_grain(model_file):\n",
    "    ldm = load_model(model_file)\n",
    "    for layer in ldm.layers:\n",
    "        layer.trainable = False\n",
    "    ldm.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])  \n",
    "    return ldm    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "589/588 [==============================] - 710s 1s/step - loss: 0.1248 - acc: 0.5319 - val_loss: 0.1189 - val_acc: 0.5299\n",
      "Epoch 2/4\n",
      "589/588 [==============================] - 710s 1s/step - loss: 0.1251 - acc: 0.5321 - val_loss: 0.1189 - val_acc: 0.5299\n",
      "Epoch 3/4\n",
      "589/588 [==============================] - 612s 1s/step - loss: 0.1252 - acc: 0.5320 - val_loss: 0.1189 - val_acc: 0.5299\n",
      "Epoch 4/4\n",
      "589/588 [==============================] - 610s 1s/step - loss: 0.1248 - acc: 0.5324 - val_loss: 0.1189 - val_acc: 0.5299\n",
      "model saved, filename: model2.h5.post.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pre_m_name = 'model2.h5.copy'\n",
    "post_m_name = 'model2.h5.post'\n",
    "data_dir = 'data_new'\n",
    "history_file = 'run_history_new'\n",
    "m = load_model_fine_grain(pre_m_name)\n",
    "history = run_model(m, data_dir) \n",
    "save_model(m, post_m_name)  \n",
    "save_history(history, history_file)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
